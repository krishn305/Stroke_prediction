# -*- coding: utf-8 -*-
"""stroke_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vMlr-UcbvUP6UPB7YIzEodIJTctkOOdY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from lightgbm import LGBMClassifier
import shap
import joblib

"""Understanding the Data"""

df = pd.read_csv("healthcare-dataset-stroke-data.csv")
df.head()

df.shape

df.info()

df.describe()

df.isnull().sum()

df['bmi'] = df['bmi'].fillna(df['bmi'].median())

df.isnull().sum()

df['stroke'].value_counts(normalize=True)
sns.countplot(x='stroke', data=df)

"""Data Balancing"""

from imblearn.over_sampling import SMOTE

X = df.drop('stroke', axis=1)
y = df['stroke']

X = pd.get_dummies(X, drop_first=True)

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

df.columns

"""Encoding and Standardisation"""

cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

df.drop(['id'], axis=1, inplace=True)

scaler = StandardScaler()
df[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])

df.columns

df.dtypes

num_cols = ['age', 'avg_glucose_level', 'bmi']

for col in num_cols:
    plt.figure(figsize=(8,4))
    sns.histplot(df[col], kde=True, color='skyblue')
    plt.title(f"Distribution of {col}")
    plt.show()

# boxoutliers
for col in num_cols:
    plt.figure(figsize=(6,3))
    sns.boxplot(x=df[col], color='orange')
    plt.title(f"Outliers in {col}")
    plt.show()

#Target Variable (Stroke)
plt.figure(figsize=(4,3))
sns.countplot(x='stroke', data=df, palette='coolwarm')
plt.title("Stroke Count")
plt.show()

stroke_rate = df['stroke'].value_counts(normalize=True) * 100
print(stroke_rate)

#Bivariate Analysis (2 Variables)
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x='stroke', y=col, data=df, palette='pastel')
    plt.title(f"{col} vs Stroke")
    plt.show()

#Mean comparision
df.groupby('stroke')[num_cols].mean()

#Multivariant
corr = df[num_cols + ['stroke']].corr()
plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

#Pairplot
sns.pairplot(df, vars=['age', 'avg_glucose_level', 'bmi'], hue='stroke', diag_kind='kde')
plt.show()

#Heatmap Including Encoded Variables
encoded_df = pd.get_dummies(df, drop_first=True)
plt.figure(figsize=(12,8))
sns.heatmap(encoded_df.corr(), cmap='coolwarm')
plt.title("Feature Correlation Heatmap (Encoded)")
plt.show()

#Statistical Insight Summary
print("Average age of stroke patients:", df[df['stroke']==1]['age'].mean())
print("Average glucose level of stroke patients:", df[df['stroke']==1]['avg_glucose_level'].mean())
print("Average BMI of stroke patients:", df[df['stroke']==1]['bmi'].mean())

df.to_csv("cleaned_stroke_data.csv", index=False)

"""Splitting Data for Training and Testing"""

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

"""Applying LGBM (Light Gradient Boosting Machine) model"""

model = LGBMClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')

params = {
    'num_leaves': [31, 50],
    'max_depth': [5, 10, -1],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200]
}
grid = GridSearchCV(LGBMClassifier(), params, cv=3, scoring='f1', verbose=1)
grid.fit(X_train, y_train)

best_model = grid.best_estimator_

print("Best Parameters:", grid.best_params_)
print("Best F1 Score:", grid.best_score_)

from sklearn.metrics import classification_report

y_pred_best = best_model.predict(X_test)
print(classification_report(y_test, y_pred_best))

sns.heatmap(confusion_matrix(y_test, y_pred_best), annot=True, fmt='d', cmap='Blues')

"""Applying SVM model"""

#Using SVM Model
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

y_pred_svm = svm_model.predict(X_test)

print("SVM Model Performance:")
print(classification_report(y_test, y_pred_svm))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues')
plt.title("SVM Confusion Matrix")
plt.show()

"""Applying Perceptron model"""

#Perceptron (Single-Layer Neural Network)
from sklearn.linear_model import Perceptron

perc_model = Perceptron(max_iter=1000, random_state=42)
perc_model.fit(X_train, y_train)

y_pred_perc = perc_model.predict(X_test)

print("Perceptron Model Performance:")
print(classification_report(y_test, y_pred_perc))
sns.heatmap(confusion_matrix(y_test, y_pred_perc), annot=True, fmt='d', cmap='Greens')
plt.title("Perceptron Confusion Matrix")
plt.show()

"""Applying Naive Bayes model"""

#Naive Bayes (GaussianNB)
from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)

print("Naive Bayes Model Performance:")
print(classification_report(y_test, y_pred_nb))
sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='d', cmap='Oranges')
plt.title("Naive Bayes Confusion Matrix")
plt.show()

"""Applying Linear Regression model"""

#Linear Regression
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

y_pred_lin = lin_reg.predict(X_test)

# Convert predictions into binary (0/1)
y_pred_lin_class = [1 if i > 0.5 else 0 for i in y_pred_lin]

print("Linear Regression Model Evaluation:")
print(classification_report(y_test, y_pred_lin_class))

sns.heatmap(confusion_matrix(y_test, y_pred_lin_class), annot=True, fmt='d', cmap='Blues')
plt.title("Linear Regression (as classifier)")
plt.show()

"""Applying Logistic Regression"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

y_pred_log = log_reg.predict(X_test)

print("Logistic Regression Model Performance:")
print(classification_report(y_test, y_pred_log))

sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Greens')
plt.title("Logistic Regression Confusion Matrix")
plt.show()

print("Accuracy:", accuracy_score(y_test, y_pred_log))

#Visualize Model Coefficients (Feature Importance)
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': log_reg.coef_[0]
}).sort_values(by='Coefficient', ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(data=feature_importance, x='Coefficient', y='Feature', palette='coolwarm')
plt.title("Logistic Regression Feature Importance")
plt.show()

"""Saving model performance summary for creating dashboard"""

import pandas as pd

# Model performance summary
model_performance = {
    'Model': ['LGBM', 'SVM', 'Linear Regression', 'Logistic Regression', 'Perceptron', 'Naive Bayes'],
    'Accuracy': [0.96, 0.85, 0.79, 0.79, 0.70, 0.64],
    'Precision': [0.96, 0.85, 0.79, 0.79, 0.70, 0.78],
    'Recall': [0.96, 0.85, 0.79, 0.79, 0.70, 0.64],
    'F1-Score': [0.96, 0.85, 0.79, 0.79, 0.70, 0.59]
}

# Convert to DataFrame
kpi_df = pd.DataFrame(model_performance)

# Display in Python
print(kpi_df)

# Export to CSV for Tableau
kpi_df.to_csv("model_kpi_summary.csv", index=False)

joblib.dump(best_model, 'stroke_prediction_model.pkl')

model = joblib.load('stroke_prediction_model.pkl')
# ==============================
# Deployment-ready simple model
# ==============================

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

features = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level']]
target = df['stroke']

X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, random_state=42
)

deploy_model = LogisticRegression(max_iter=1000)
deploy_model.fit(X_train, y_train)

joblib.dump(deploy_model, "stroke_prediction_model.pkl")
